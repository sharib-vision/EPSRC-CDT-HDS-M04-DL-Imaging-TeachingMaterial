{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDS-M05: Deep Learning\n",
    "\n",
    "Module Leaders: Bartek Papiez and Sharib Ali\n",
    "\n",
    "Practical Leader: Sharib Ali, PhD\n",
    "\n",
    "\n",
    "\n",
    "### Required packages\n",
    "[1] [numpy](http://www.numpy.org) is package for scientific computing with python\n",
    "\n",
    "[2] [h5py](http://www.h5py.org) is package to interact with compactly stored dataset\n",
    "\n",
    "[3] [matplotlib](http://matplotlib.org) can be used for plotting graphs in python\n",
    "\n",
    "\n",
    "### Ingrediants of deep learning explained with logistic regression\n",
    "\n",
    "You will build a logistic regression classifier to recognize cats. Logistic regression can be considered as a very small neural network.\n",
    "\n",
    "\n",
    "What will you learn here?\n",
    "\n",
    "**Mathematical expression of the algorithm**:\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n",
    "\n",
    "**Loss or error function:** $ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$\n",
    "\n",
    "If $ y =1: \\mathcal{L}(\\hat{y}, y) = -log(\\hat{y})$ i.e. we want to make $\\hat{y}$ large\n",
    "\n",
    "If $ y =0: \\mathcal{L}(\\hat{y}, y) = -log(1-\\hat{y})$ i.e. we want to make $\\hat{y}$ as small as possible\n",
    "\n",
    "**Cost function:**\n",
    "The cost is then computed by summing over all training examples:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Build the ingrediants\n",
    "\n",
    "[1] Activation functions\n",
    "\n",
    "[2] Forward and backward propagation *(includes weights, activation functions and derivatives)*\n",
    "\n",
    "[3] Optimiser *(includes cost and its derivates)*\n",
    "\n",
    "[4] Predictor *(activation function of the output layer)*\n",
    "\n",
    "[5] **The model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"sigmoid(-0.1) = \" + str(sigmoid(-0.1)))\n",
    "print (\"sigmoid(0.5) = \" + str(sigmoid(0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO Implement a relu activation function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_with_parameters_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    Initialize your weight matrix 'w' (vector in case of single neuron case) and bias 'b' which is a single scalar value\n",
    "    in single neuron case other wise (no_of_neurons , 1)\n",
    "    \n",
    "    Hint => dimension of w (dim): should be equal to number of input variables\n",
    "    \n",
    "    This function should return w and b\n",
    "    \"\"\"\n",
    "    w = np.zeros(shape=(dim, 1))\n",
    "    b = 0\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 3\n",
    "w, b = initialize_with_parameters_with_zeros(dim)\n",
    "print (\"w = \" + str(w))\n",
    "print (\"b = \" + str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward and Backward propagation:**\n",
    "\n",
    "- You get X (vectorize your data, in python data.flatten(),  $w\\times h \\times ch$)\n",
    "- $z = w^{T} X + b$\n",
    "- You compute $A = \\sigma(z) = (a^{(0)}, a^{(1)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- You calculate the cost function (calculated over all input values) => $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n",
    "\n",
    "**Gradient descent:** \n",
    "\n",
    "We want to find w and b so that it minimize $J(w, b)$\n",
    "\n",
    "Note in single neuron case with one layer only, your computed activation is the output so 'A' here is the activation output directly. However, remember that you can minimize cost only after computing loss for all the input variables.\n",
    "\n",
    "\n",
    "- Find slopes:\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$\n",
    "\n",
    "- Update your parameters:\n",
    "\n",
    "$$ w:= w - \\alpha \\frac{\\partial J}{\\partial w}$$\n",
    "$$ b:= b - \\alpha \\frac{\\partial J}{\\partial b}$$\n",
    "\n",
    "$\\alpha$ here is the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward_propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (e.g., containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost/J -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "#   # get the size of your input data\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Step 1: Forward propagation\n",
    "    # compute activation\n",
    "    A = sigmoid(np.dot(w.T, X) + b) \n",
    "    # remember: your activation is the output here but only if you have one neuron other wise it is activation at A^[L-1]    \n",
    "    # compute cost J (we are not using mean square error as this is non-convex in binary classification,\n",
    "    # so, we use logistic regression log likelihoods)\n",
    "    cost = (- 1 / m) * np.sum(Y * np.log(A) + (1 - Y) * (np.log(1 - A)))  \n",
    "    \n",
    "    # Step 2: Backward propagation (compute your derivatives of the computed cost see derivation above (slope))\n",
    "    dw = (1 / m) * np.dot(X, (A - Y).T)\n",
    "    db = (1 / m) * np.sum(A - Y)\n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "#     return gradients and cost \n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimization** \n",
    "\n",
    "- You have initialized your parameters.\n",
    "- You are also able to compute a cost function and its gradient.\n",
    "- Now, you want to update the parameters using gradient descent. For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        # Cost and gradient calculation \n",
    "        grads, cost = forward_backward_propagate(w, b, X, Y)\n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update your parameters (i.e. new set of w and b are found)\n",
    "        w = w - learning_rate * dw  \n",
    "        b = b - learning_rate * db\n",
    "        \n",
    "        # Record the costs every 100 iterations\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" % (i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction**\n",
    "\n",
    "We have implemented forward-, backward-propagations and updates. Now, we want to predict.\n",
    "\n",
    "1. Calculate $\\hat{Y} = A = \\sigma(w^T X + b)$\n",
    "\n",
    "2. Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector `Y_prediction`. If you wish, you can use an `if`/`else` statement in a `for` loop (though there is also a way to vectorize this). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    #   Remember its always good idea to keep a sigmoid activation at output layer as now you will require to predict the labels\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "   \n",
    "    #   For number of activations predict labels and save each prediction to be used later for computing training and validation accuracy \n",
    "    for i in range(A.shape[1]):\n",
    "        Y_prediction[0, i] = 1 if A[0, i] > 0.5 else 0\n",
    "\n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The model**\n",
    "\n",
    "Implement the model function. Use the following notation:\n",
    "    - Y_prediction for your predictions on the test set\n",
    "    - Y_prediction_train for your predictions on the train set\n",
    "    - w, costs, grads for the outputs of optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize parameters with zeros (this is vector for single neuron but a matrix for multi-neuron network)\n",
    "    w, b = initialize_with_parameters_with_zeros(X_train.shape[0])\n",
    "\n",
    "    # Gradient descent\n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Predict test/train set examples \n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare your data**\n",
    "\n",
    "You will play with cat vs non-cat dataset provided to you. However, you have to first make sure that the data have correct dimensions. Normalize by dividing with 255 if you are using images (as for example this case). You will do the same in the remaining examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "# training set\n",
    "train_dataset = h5py.File('datasets/cat-non-cat/train_catvnoncat.h5', \"r\")\n",
    "train_x = np.array (train_dataset[\"train_set_x\"][:])\n",
    "train_y = np.array( train_dataset[\"train_set_y\"][:])\n",
    "\n",
    "# testing set\n",
    "test_dataset = h5py.File('datasets/cat-non-cat/test_catvnoncat.h5', \"r\")\n",
    "test_x = np.array (test_dataset[\"test_set_x\"][:])\n",
    "test_y = np.array( test_dataset[\"test_set_y\"][:])\n",
    "\n",
    "# class list\n",
    "classes = np.array(test_dataset[\"list_classes\"][:])\n",
    "\n",
    "# reshape labels\n",
    "trainY = train_y.reshape(1, train_y.shape[0])\n",
    "testY = test_y.reshape((1, test_y.shape[0]))\n",
    "\n",
    "# warning: check dimensions and be sure of it!!!\n",
    "print('classes are:', classes)\n",
    "print('train_x:', train_x.shape)\n",
    "print('train_y (training labels):', trainY.shape)\n",
    "print('number of test labels:', testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_flatten= train_x.reshape(train_x.shape[0], -1).T\n",
    "test_x_flatten= test_x.reshape(test_x.shape[0], -1).T\n",
    "\n",
    "print('flattened training data (train_x_flatten): ', train_x_flatten.shape)\n",
    "print('flattened testing data (test_x_flatten): ', test_x_flatten.shape)\n",
    "\n",
    "trainX = train_x_flatten/255.\n",
    "testX = test_x_flatten/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = model(trainX, trainY, testX, testY, num_iterations = 2000, learning_rate = 0.005, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a picture that was wrongly classified.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "index = 20 # try 45, 10, 20 or whatever you wish to (note: size of your test set is only 50)\n",
    "plt.imshow(testX[:,index].reshape((64, 64, 3)))\n",
    "prediction_label= int(d['Y_prediction_test'][0, index])\n",
    "print (\"y = \" + str(testY[0, index]) + \", you predicted that it is a \\\"\" + classes[prediction_label].decode(\"utf-8\") +  \"\\\" picture.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve (with costs)\n",
    "costs = np.squeeze(d['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Play with different learning rates and see how these effect your learning**\n",
    "\n",
    "In order for Gradient Descent to work you must choose the learning rate wisely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "models = {}\n",
    "for i in learning_rates:\n",
    "    print (\"learning rate is: \" + str(i))\n",
    "    models[str(i)] = model(trainX, trainY, testX, testY, num_iterations = 1500, learning_rate = i, print_cost = False)\n",
    "    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n",
    "\n",
    "for i in learning_rates:\n",
    "    plt.plot(np.squeeze(models[str(i)][\"costs\"]), label= str(models[str(i)][\"learning_rate\"]))\n",
    "\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations')\n",
    "\n",
    "legend = plt.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Thanks for completing this lesson!</h3>\n",
    "\n",
    "*Courtesy: Andrew Ng. (Coursera module instructor)*\n",
    "\n",
    "Any comments or feedbacks, please send to [Sharib Ali](sharib.ali@eng.ox.ac.uk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
