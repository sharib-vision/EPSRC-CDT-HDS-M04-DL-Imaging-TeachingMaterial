{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDS-M05: Deep Learning (Exercise III)\n",
    "\n",
    "**Practical leader and prepared by:** Sharib Ali, PhD\n",
    "\n",
    "[Setup instructions](https://github.com/sharibox/tutorial/blob/master/HDS-CDT_DL.md)\n",
    "\n",
    "### Required packages\n",
    "\n",
    "[1] [matplotlib](http://matplotlib.org) can be used for plotting graphs in python\n",
    "\n",
    "[2] [pytorch](https://pytorch.org/docs/stable/index.html) is library widely used for bulding deep-learning frameworks\n",
    "\n",
    "[3] [Tensorboard](https://pytorch.org/docs/stable/tensorboard.html) is used to visualise your training and validation loss and accuracy development - It is important to observe it!!!\n",
    "\n",
    "[4] [TorchVision](https://pytorch.org/vision/stable/index.html) you can use available datasets that include one you are required to use in this tutorial (CIFAR10) and also use augmentations (meaning you can increase data variability for improved accuracy and model generalisation)\n",
    "\n",
    "[5] [Scikit-learn] Metrics\n",
    "\n",
    "**Reference:** \n",
    "\n",
    "[1] Original U-Net paper: https://arxiv.org/pdf/1505.04597.pdf\n",
    "\n",
    "[2] Link to presentation: https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/\n",
    "\n",
    "\n",
    "**What will you learn here?**\n",
    "\n",
    "- You wil learn how to load your custom data for semantic segmentation task\n",
    "\n",
    "- You will make an encoder-decoder U-Net architecture \n",
    "\n",
    "- You will learn how to group repeated blocks into one while designing U-Net\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/EtyQs.png\" style=\"width:800px;height:400px;\">\n",
    "<caption><center> <u>Figure</u>: U-Net architecture for image segmentation.</center></caption>\n",
    "\n",
    "**Concentrate on:**\n",
    "\n",
    "[1] Encoder layers (this is similar to your classification exercise)\n",
    "\n",
    "[2] Decoder layers (upscaling of images occur here as you want to obtain per-pixel segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision \n",
    "from torch import nn\n",
    "import numpy as np\n",
    "# always check your version\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New : We will setup a seed for reproducibility (ask tutor if you do not understand it)\n",
    "- Setting a seed helps to regenerate the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random_seed= 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and pre-processing\n",
    "**Steps**\n",
    "\n",
    "[1] Load data - data is provided already available for you to use in ``/cdtshared/Cohort-2021/HDS-M05/segmentation/MontgomerySet``\n",
    "- Images are in folder ``/cdtshared/Cohort-2021/HDS-M05/segmentation/MontgomerySet/CXR_png``\n",
    "- Corresponding masks:\n",
    "    - Left Mask in ``/cdtshared/Cohort-2021/HDS-M05/segmentation/MontgomerySet/ManualMask/leftMask``\n",
    "    - Right Mask in ``/cdtshared/Cohort-2021/HDS-M05/segmentation/MontgomerySet/ManualMask/rightMask``\n",
    "- Data format and size: All are in ``.png`` and of variable size (please resize to 256x256) for your practicals\n",
    "\n",
    "[2] Transform --> Normalise your data - mean and std (e.g., if color then normalise all three channels)\n",
    "e.g., torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "\n",
    "[3] Transform --> Always convert data to ToTensor (you can do **step 1, 2 and 3** in one line as done in this tutorial)\n",
    "\n",
    "[4] New: split your data into **train and validation set**\n",
    "\n",
    "[4] Make [DataLoaders](https://pytorch.org/docs/stable/data.html): It represents a Python iterable over a dataset\n",
    "\n",
    "[5] Identify labels \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms \n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing transform for step 2 and step 3\n",
    "# mean = (0.5, 0.5, 0.5)\n",
    "# std = (0.5, 0.5, 0.5)\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize(224),   # Note: here you will resize image to 224 that is input to the AlexNet\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean, std)\n",
    "#     ])\n",
    "\n",
    "from PIL import Image # For reading your custom data images\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Load data and include prepared transform (Remember to apply same transform to both train \n",
    "class mySegmentationData(object):\n",
    "    def __init__(self, root, transforms = None):\n",
    "        self.root = root\n",
    "        self._eval = eval\n",
    "        self.transforms = transforms\n",
    "        self.build_dataset()\n",
    "        \n",
    "    def build_dataset(self):   \n",
    "        self.imgs = os.path.join(self.root, \"CXR_png\")\n",
    "        self.masks_L = os.path.join(self.root, \"ManualMask/leftMask\")\n",
    "        self.masks_R = os.path.join(self.root, \"ManualMask/rightMask\")\n",
    "        \n",
    "        self._images = glob.glob(self.imgs + \"/*.png\")\n",
    "        self._labels = glob.glob(self.masks_L  + \"/*.png\")\n",
    "#         print(len(self._images))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "    \n",
    "        img = Image.open(self._images[idx]).convert(\"L\").resize((256,256), resample=0)\n",
    "        mask = Image.open(self._labels[idx]).convert(\"RGB\").resize((256, 256), resample=0)\n",
    "        mask = Image.fromarray((np.asarray(mask)[:,:,0].astype('float32')/255.0))\n",
    "#         Image.fromarray((np.asarray(target)[:,:,0].astype('float32')/255.0).astype('uint8'))\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "            mask = self.transforms(mask)\n",
    "   \n",
    "        return img, mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New: You will split train data into train and val set (say 90-10) \n",
    "- This step is crucial to identify under- and over-fitting problems \n",
    "- Later, we will visualise performance on both train and test online during training (using tensorboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'albumentations'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7221cc35c9d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# transforms.Resize((256,256))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0malbumentations\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0malbumentations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugmentations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0malbumentations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mToTensorV2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'albumentations'"
     ]
    }
   ],
   "source": [
    "# Step: Split between train and valset from the overall trainset\n",
    "transform = transforms.Compose([ transforms.ToTensor()])\n",
    "# transforms.Resize((256,256))\n",
    "\n",
    "import albumentations as A\n",
    "import albumentations.augmentations.functional as F\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(256, 256),\n",
    "        A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=30, p=0.5),\n",
    "        A.RGBShift(r_shift_limit=25, g_shift_limit=25, b_shift_limit=25, p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
    "        A.Normalize(mean=(0.5), std=(0.5)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(256, 256),\n",
    "        A.Normalize(mean=(0.5), std=(0.5)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainset = mySegmentationData(root='MontgomerySet/', transforms=train_transform)\n",
    "valset = mySegmentationData(root='MontgomerySet/', transforms=val_transform)\n",
    "#  same as you did in classification (here we are doing )\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "val_percentage = 0.1\n",
    "num_train = len(trainset)\n",
    "\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(val_percentage * num_train))\n",
    "\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "print(len(train_sampler))\n",
    "\n",
    "# Now create data loaders (same as before)\n",
    "# Now we need to create dataLoaders that will allow to iterate during training\n",
    "batch_size = 2 # create batch-based on how much memory you have and your data size\n",
    "\n",
    "traindataloader = DataLoader(trainset, batch_size=batch_size, sampler=train_sampler, num_workers=0)\n",
    "valdataloader = DataLoader(valset, batch_size=batch_size, sampler=valid_sampler,\n",
    "            num_workers=0,)\n",
    "\n",
    "# testdataloader = DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of training samples:', len(traindataloader))\n",
    "print('Number of validation samples:', len(valdataloader))\n",
    "# print('Number of testing samples:', len(testdataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look into data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to unnormalise images and using transpose to change order to [H, W, Channel] \n",
    "def imshow(img):\n",
    "    # TODO: unnormalize if needed\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "# always check the shape of your training data\n",
    "dataiter = iter(traindataloader)\n",
    "images, masks = dataiter.next()\n",
    "\n",
    "# show images \n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "imshow(torchvision.utils.make_grid(masks))\n",
    "# print labels\n",
    "# print(' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks[0].max()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your UNet model\n",
    "- Please note that your input image size will make difference on your hard-coded feature sizes in FC-layer\n",
    "- Always be aware of what size input is used, here for convenience we will follow the original paper and reshape image to 224x224x3 \n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/EtyQs.png\" style=\"width:800px;height:400px;\">\n",
    "<caption><center> <u>Figure</u>: U-Net architecture for image segmentation.</center></caption>\n",
    "\n",
    "**Hint**\n",
    "- You will make an encoder-decoder U-Net architecture \n",
    "    - Make a baseblock with --> two convolution with kernel_size 3 and padding 1 with relu \n",
    "    - Make another output block with 1 convolution\n",
    "    - For encoder you will use maxpooling with kernel 2\n",
    "    - For decoder you will use upsampling using nn.Upsample (scale_factor=2)\n",
    "    \n",
    "\n",
    "- You will learn how to group repeated blocks into one while designing U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic convolution layer -- 2 convs concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_layer(nn.Module):\n",
    "    def __init__(self, channel_in, channel_out):\n",
    "        super().__init__()\n",
    "        self.conv_l = nn.Sequential(\n",
    "            nn.Conv2d(channel_in, channel_out, kernel_size = 3, padding = 1),\n",
    "            DropBlock2D(),\n",
    "            nn.BatchNorm2d(channel_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv_l(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )   \n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, inputchannel, n_class):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.dconv_down1 = double_conv(inputchannel, 64)\n",
    "        self.dconv_down2 = double_conv(64, 128)\n",
    "        self.dconv_down3 = double_conv(128, 256)\n",
    "        self.dconv_down4 = double_conv(256, 512)        \n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)        \n",
    "        \n",
    "        self.dconv_up3 = double_conv(256 + 512, 256)\n",
    "        self.dconv_up2 = double_conv(128 + 256, 128)\n",
    "        self.dconv_up1 = double_conv(128 + 64, 64)\n",
    "        \n",
    "        self.conv_last = nn.Conv2d(64, n_class,1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        x = self.maxpool(conv1)\n",
    "\n",
    "        conv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool(conv2)\n",
    "        \n",
    "        conv3 = self.dconv_down3(x)\n",
    "        x = self.maxpool(conv3)   \n",
    "        \n",
    "        x = self.dconv_down4(x)\n",
    "        \n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "        \n",
    "        x = self.dconv_up3(x)\n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv2], dim=1)       \n",
    "\n",
    "        x = self.dconv_up2(x)\n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv1], dim=1)   \n",
    "        \n",
    "        x = self.dconv_up1(x)\n",
    "        \n",
    "        out = self.conv_last(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare an optimizer, set learning rate, and you loss function\n",
    "- Here you will use model.train and use gradients\n",
    "- Also, you will use criterion to compute loss \n",
    "- Compute metric to know how well it is performing\n",
    "- save them to display mean for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call you model \n",
    "from torch import optim\n",
    "model = UNet(inputchannel=1, n_class=1)\n",
    "lr = 0.001\n",
    "# optimiser = optim.Adam(model.parameters(), lr=lr)\n",
    "# Optimiser\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr = lr, weight_decay = 1e-8)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# optimiser = torch.optim.RMSprop(model.parameters(), lr = lr, weight_decay = 1e-8, momentum=0.9)\n",
    "# TODO: you can try with different loss function\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare accuracy computation to know how your training is going \n",
    "\n",
    "[1] Loss function is important to keep track (mostly you minimise it, i.e. it should go down)\n",
    "\n",
    "[2] Accuracy in classification is important and you want higher accuracy\n",
    "\n",
    "[3] You can use TensorBoard to visualize both - on new terminal do below\n",
    "\n",
    "```shell\n",
    "ssh -L 8889:127.0.0.1:8889 $user@bdicdtvm01.bmrc.ox.ac.uk \n",
    "$ ml Anaconda3\n",
    "$ source activate base\n",
    "\n",
    "- run your training below and then do this while waiting:\n",
    "$ tensorboard --logdir runs/ --port 8889\n",
    "$ http://127.0.0.1:8889/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metrics\n",
    "def dice(a, b):\n",
    "    \"\"\"Calculate dice score for each image in tensor\"\"\"\n",
    "    # a and b are tensors of shape (B, C, H, W)\n",
    "    # Sum over last two axes (H and W i.e. each image)\n",
    "    return 2*(a*b).sum(axis=[-2, -1])/(a + b).sum(axis=[-2,-1]).type(torch.float32)\n",
    "\n",
    "def mask_out(out):\n",
    "    \"\"\"Mask tensor/array with 0 threshold\"\"\"\n",
    "    # Need to binarize the output to be able to calculate dice score\n",
    "    return out > 0\n",
    "\n",
    "def get_dice_arr(out, label):\n",
    "    \"\"\"Get dice score for each image in the batch for each mask seperately\"\"\"\n",
    "    # Output is shape (B, C)\n",
    "    return dice(mask_out(out), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4] Run your training loop with optimiser trying to minimise your cost/loss, dont forget to backpropagate your loss\n",
    "model.to(device)\n",
    "model.train()\n",
    "# Tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# Writer will output to ./runs/ directory by default\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# define no. of epochs you want to loop \n",
    "epochs = 2\n",
    "log_interval = 10 # for visualising your iterations \n",
    "\n",
    "# New: savining your model depending on your best val score\n",
    "best_valid_loss = float('inf')\n",
    "ckptFileName = 'UNet_CKPT_best.pt'\n",
    "for epoch in range(epochs):\n",
    "    train_loss, valid_loss, train_dsc,val_dsc  = [], [], [], []\n",
    "    \n",
    "    for batch_idx, (data, label) in enumerate(traindataloader):\n",
    "        # initialise all your gradients to zer\n",
    "\n",
    "        label = label.to(device)\n",
    "        optimiser.zero_grad()\n",
    "        out = model(data.to(device))\n",
    "\n",
    "        loss = criterion(out, label)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "        # append\n",
    "        train_loss.append(loss.item())\n",
    "        acc_1 = get_dice_arr(out, label.to(device))\n",
    "        \n",
    "        \n",
    "        print(acc_1.mean(axis=0).detach().cpu().numpy())\n",
    "        train_dsc.append(acc_1.mean(axis=0).detach().cpu().numpy())\n",
    "        \n",
    "        if (batch_idx % log_interval) == 0:\n",
    "            print('Train Epoch is: {}, train loss is: {:.6f}'.format(epoch, np.mean(train_loss)))\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                for i, (data, label) in enumerate(valdataloader):\n",
    "                    data, label = data.to(device), label.to(device)\n",
    "                    out = model(data)\n",
    "                    loss = criterion(out, label.to(device))\n",
    "                    acc_1 = get_dice_arr(out, label.to(device))\n",
    "\n",
    "                    # append\n",
    "                    val_dsc.append(acc_1.mean(axis=0).detach().cpu().numpy())\n",
    "                    valid_loss.append(loss.item())\n",
    "    \n",
    "            print('Val Epoch is: {}, val loss is: {:.6f}'.format(epoch, np.mean(valid_loss)))\n",
    "    \n",
    "    #New --> save your checkpoint every epoch if best\n",
    "    if np.mean(valid_loss) < best_valid_loss:\n",
    "        best_valid_loss = np.mean(valid_loss)\n",
    "        print('saving my model, improvement in validation loss achieved...')\n",
    "        torch.save(model.state_dict(), ckptFileName)\n",
    "        \n",
    "        \n",
    "    # every epoch write the loss and accuracy (these you can see plots on tensorboard)        \n",
    "    writer.add_scalar('UNet/train_loss', np.mean(train_loss), epoch)\n",
    "#     writer.add_scalar('UNet/train_accuracy', np.mean(train_top1), epoch)\n",
    "    \n",
    "    # New --> add plot for your val loss and val accuracy\n",
    "    writer.add_scalar('UNet/val_loss', np.mean(valid_loss), epoch)\n",
    "#     writer.add_scalar('UNet/val_accuracy', np.mean(val_top1), epoch)\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: compute the accuracy of your model on validation set --> You are required to use dice similarity coefficient\n",
    "total = 0\n",
    "model.eval()\n",
    "dataiter = iter(valdataloader)\n",
    "images, masks = dataiter.next()\n",
    "output = model(images.to(device))\n",
    "\n",
    "\n",
    "# show images \n",
    "# imshow(torchvision.utils.make_grid(images))\n",
    "# imshow(torchvision.utils.make_grid(output))\n",
    "\n",
    "output[0].max()\n",
    "# with torch.no_grad():\n",
    "#     for data in testdataloader:\n",
    "#         image, labels = data\n",
    "#         output = model(image.to(device))\n",
    "#         _, preds_tensor = torch.max(output, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively you can compute for each class seperately as well \n",
    "#(taken from: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testdataloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images.to(device))\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label.to(device) == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
    "                                                   accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New => Inference/testing on saved checkpoint\n",
    "- Load your trained model and apply test on testdataloader \n",
    "- If you have checkpoint file (trained weights) you can simply call below for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckptFileName = 'alexNet_CKPT_best.pt'\n",
    "# load the saved weights\n",
    "model.load_state_dict(torch.load(ckptFileName))\n",
    "\n",
    "# Apply testing (same as validation above)\n",
    "test_preds = []\n",
    "labels = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for i, batch in enumerate(testdataloader):\n",
    "        img, label = batch\n",
    "        img, label = img.to(device, dtype = torch.float), label.to(device, dtype = torch.long)\n",
    "        output = model(img)\n",
    "        output = output.detach().cpu().numpy()\n",
    "        test_preds.extend(np.argmax(output, 1))\n",
    "        labels.extend(label.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- New => Use scikit-learn to see the confuse matrix/compute accuracy (recall ML-03)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(labels, test_preds))\n",
    "print('Accuracy score: %f' % accuracy_score(labels, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving your network peerformance\n",
    "\n",
    "- Train for larger batch size and epochs (longer)\n",
    "- Add data augmentation provided in transforms (https://pytorch.org/vision/stable/transforms.html) -- Ask tutor if you  are confused \n",
    "- Save your training with augmentation as ``my_AlexNet_withAug.pt``\n",
    "- Tune your hyperparameters and decay rate\n",
    "- Add your tensorboard outputs for training and validation (loss and accuracy) as image files\n",
    "\n",
    "\n",
    "#### Exercise: Perform above improvements on CIFAR10 \n",
    "You can also refer to: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "- You can use this ipython notebook to do this (**Assignment to be submmitted**)\n",
    "- Due on **Wednesday 3rd November, 2021 (11:59 PM)** (*You will be graded for this exercise*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Thanks for completing this lesson!</h3>\n",
    "\n",
    "Any comments or feedbacks and your solution to exercise, please send to [Sharib Ali](sharib.ali@eng.ox.ac.uk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
